## 上下文

当前年俗知识问答系统使用RAG流程从知识库中检索相关信息并生成回答。当知识库中找不到答案时，系统只能返回固定的"抱歉，我暂时没有关于这个问题的信息。"，用户体验较差。为了提高系统的覆盖范围和用户体验，需要集成LLM后端，当知识库检索结果为空时，调用LLM生成回答。

## 目标 / 非目标

**目标：**
- 集成OpenAI API作为LLM后端
- 当知识库中找不到答案时，调用LLM生成回答
- 实现LLM回答的质量控制和过滤
- 提供LLM配置管理，支持API密钥和模型参数配置
- 保持系统的响应速度和稳定性

**非目标：**
- 替换现有的知识库和RAG流程
- 支持多种LLM后端（仅集成OpenAI API）
- 实现复杂的LLM微调或自定义模型

## 决策

### 1. LLM后端选择
- **决策**：使用OpenAI API作为LLM后端
- **理由**：OpenAI API提供了强大的语言模型能力，支持中文生成，文档完善，集成简单
- **替代方案**：
  - 本地部署开源模型（如Llama 3）：需要更多的硬件资源，部署复杂
  - 其他云服务提供商（如Anthropic Claude）：集成复杂度相似，但OpenAI的模型在中文处理上表现良好

### 2. 集成方式
- **决策**：在RAG流程中添加LLM回退机制
- **理由**：当知识库检索结果为空时，调用LLM生成回答，保持现有流程的完整性
- **替代方案**：
  - 完全重构为基于LLM的系统：会失去知识库的准确性和可控性
  - 并行调用知识库和LLM：增加系统复杂度和响应时间

### 3. 配置管理
- **决策**：使用环境变量存储OpenAI API密钥，使用配置文件存储模型参数
- **理由**：环境变量提供了安全的方式存储敏感信息，配置文件便于管理模型参数
- **替代方案**：
  - 硬编码在代码中：不安全，不利于部署
  - 数据库存储：增加系统复杂度

### 4. 质量控制
- **决策**：实现基本的LLM回答过滤，拒绝不适当的内容
- **理由**：确保系统生成的回答符合伦理和法律要求
- **替代方案**：
  - 使用OpenAI的内容过滤API：增加API调用成本
  - 不进行过滤：存在生成不适当内容的风险

## 风险 / 权衡

### 1. API调用成本
- **风险**：频繁调用OpenAI API可能会产生较高的成本
- **缓解措施**：
  - 仅在知识库中找不到答案时调用LLM
  - 设置API调用频率限制
  - 考虑使用成本较低的模型（如gpt-3.5-turbo）

### 2. 响应时间
- **风险**：调用LLM可能会增加系统的响应时间
- **缓解措施**：
  - 优化API调用参数，减少生成的token数量
  - 实现异步调用，提高用户体验
  - 为LLM回答添加缓存机制

### 3. 回答质量
- **风险**：LLM生成的回答可能不够准确或与年俗知识不符
- **缓解措施**：
  - 在提示词中明确要求基于准确的年俗知识生成回答
  - 对LLM生成的回答进行后处理，确保符合系统的风格和要求
  - 定期评估LLM回答的质量，调整提示词和参数

### 4. 安全风险
- **风险**：API密钥泄露可能导致未授权的API调用和额外成本
- **缓解措施**：
  - 使用环境变量存储API密钥，避免硬编码
  - 为API密钥设置适当的权限和限制
  - 定期轮换API密钥

## 实现架构

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│                 │     │                 │     │                 │
│  User Interface │────▶│  RAG Workflow   │────▶│  Knowledge Base │
│                 │◀────│                 │◀────│                 │
└─────────────────┘     └─────────────────┘     └─────────────────┘
                              │
                              │  知识库无结果
                              ▼
                        ┌─────────────────┐
                        │                 │
                        │  LLM Backend   │
                        │                 │
                        └─────────────────┘
                              │
                              ▼
                        ┌─────────────────┐
                        │                 │
                        │  回答生成      │
                        │                 │
                        └─────────────────┘
```

## 实现步骤

1. **添加依赖**：安装OpenAI Python SDK
2. **创建LLM后端模块**：实现OpenAI API调用功能
3. **修改RAG流程**：在知识库检索结果为空时，调用LLM生成回答
4. **实现配置管理**：支持API密钥和模型参数配置
5. **添加质量控制**：实现LLM回答的过滤和后处理
6. **测试集成**：验证LLM后端集成的功能和性能
7. **部署**：更新系统配置，部署到生产环境

## 开放问题

- **提示词优化**：如何设计最优的提示词，确保LLM生成准确的年俗知识回答
- **成本控制**：如何在保证回答质量的同时，控制API调用成本
- **性能优化**：如何减少LLM调用的响应时间，提高用户体验
- **监控和评估**：如何监控LLM回答的质量，及时发现和解决问题